[2024-10-27T19:47:09.168+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: holiday_etl_pipeline.extract_to_landing scheduled__2024-10-26T00:00:00+00:00 [queued]>
[2024-10-27T19:47:09.176+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: holiday_etl_pipeline.extract_to_landing scheduled__2024-10-26T00:00:00+00:00 [queued]>
[2024-10-27T19:47:09.176+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 2
[2024-10-27T19:47:09.192+0000] {taskinstance.py:2191} INFO - Executing <Task(_PythonDecoratedOperator): extract_to_landing> on 2024-10-26 00:00:00+00:00
[2024-10-27T19:47:09.197+0000] {standard_task_runner.py:60} INFO - Started process 83 to run task
[2024-10-27T19:47:09.201+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'holiday_etl_pipeline', 'extract_to_landing', 'scheduled__2024-10-26T00:00:00+00:00', '--job-id', '22', '--raw', '--subdir', 'DAGS_FOLDER/holidays_orchestror.py', '--cfg-path', '/tmp/tmp2r_6nd9f']
[2024-10-27T19:47:09.204+0000] {standard_task_runner.py:88} INFO - Job 22: Subtask extract_to_landing
[2024-10-27T19:47:09.240+0000] {task_command.py:423} INFO - Running <TaskInstance: holiday_etl_pipeline.extract_to_landing scheduled__2024-10-26T00:00:00+00:00 [running]> on host 09302ea91fa9
[2024-10-27T19:47:09.296+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='holiday_etl_pipeline' AIRFLOW_CTX_TASK_ID='extract_to_landing' AIRFLOW_CTX_EXECUTION_DATE='2024-10-26T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-10-26T00:00:00+00:00'
[2024-10-27T19:47:33.060+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/holidays_orchestror.py", line 39, in extract_holidays
    return HolidayLandingOrchestrator.extract_holidays()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/scripts/landing/holliday_landing.py", line 254, in extract_holidays
    extractor = HolidayLanding()
                ^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/scripts/landing/holliday_landing.py", line 62, in __init__
    self.spark = SparkSessionManager.get_session()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/conf/spark_session.py", line 15, in get_session
    cls._instance = spark.getOrCreate()
                    ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/sql/session.py", line 477, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/context.py", line 512, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/context.py", line 198, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/context.py", line 432, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
                                       ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/java_gateway.py", line 106, in launch_gateway
    raise RuntimeError("Java gateway process exited before sending its port number")
RuntimeError: Java gateway process exited before sending its port number
[2024-10-27T19:47:33.140+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=holiday_etl_pipeline, task_id=extract_to_landing, execution_date=20241026T000000, start_date=20241027T194709, end_date=20241027T194733
[2024-10-27T19:47:33.178+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 22 for task extract_to_landing (Java gateway process exited before sending its port number; 83)
[2024-10-27T19:47:33.206+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-10-27T19:47:33.244+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-10-27T20:25:59.344+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: holiday_etl_pipeline.extract_to_landing scheduled__2024-10-26T00:00:00+00:00 [queued]>
[2024-10-27T20:25:59.350+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: holiday_etl_pipeline.extract_to_landing scheduled__2024-10-26T00:00:00+00:00 [queued]>
[2024-10-27T20:25:59.350+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 2
[2024-10-27T20:25:59.362+0000] {taskinstance.py:2191} INFO - Executing <Task(_PythonDecoratedOperator): extract_to_landing> on 2024-10-26 00:00:00+00:00
[2024-10-27T20:25:59.367+0000] {standard_task_runner.py:60} INFO - Started process 82 to run task
[2024-10-27T20:25:59.370+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'holiday_etl_pipeline', 'extract_to_landing', 'scheduled__2024-10-26T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/holidays_orchestror.py', '--cfg-path', '/tmp/tmpdmd671hu']
[2024-10-27T20:25:59.373+0000] {standard_task_runner.py:88} INFO - Job 3: Subtask extract_to_landing
[2024-10-27T20:25:59.412+0000] {task_command.py:423} INFO - Running <TaskInstance: holiday_etl_pipeline.extract_to_landing scheduled__2024-10-26T00:00:00+00:00 [running]> on host be01200d7c31
[2024-10-27T20:25:59.467+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='holiday_etl_pipeline' AIRFLOW_CTX_TASK_ID='extract_to_landing' AIRFLOW_CTX_EXECUTION_DATE='2024-10-26T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-10-26T00:00:00+00:00'
[2024-10-27T20:26:14.500+0000] {local_task_job_runner.py:302} WARNING - State of this instance has been externally set to failed. Terminating instance.
[2024-10-27T20:26:14.502+0000] {process_utils.py:131} INFO - Sending 15 to group 82. PIDs of all processes in the group: [84, 82]
[2024-10-27T20:26:14.503+0000] {process_utils.py:86} INFO - Sending the signal 15 to group 82
[2024-10-27T20:26:14.504+0000] {taskinstance.py:2450} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-10-27T20:26:14.504+0000] {spark_session.py:59} ERROR - Erro ao criar sessão Spark: Task received SIGTERM signal
[2024-10-27T20:26:14.505+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/opt/airflow/dags/conf/spark_session.py", line 48, in get_session
    cls._instance = builder.getOrCreate()
                    ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/sql/session.py", line 477, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/context.py", line 512, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/context.py", line 198, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/context.py", line 432, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
                                       ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/java_gateway.py", line 103, in launch_gateway
    time.sleep(0.1)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 2452, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/holidays_orchestror.py", line 39, in extract_holidays
    return HolidayLandingOrchestrator.extract_holidays()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/scripts/landing/holliday_landing.py", line 255, in extract_holidays
    extractor = HolidayLanding()
                ^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/scripts/landing/holliday_landing.py", line 67, in __init__
    self.spark = SparkSessionManager.get_session()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/conf/spark_session.py", line 60, in get_session
    raise Exception(error_msg)
Exception: Erro ao criar sessão Spark: Task received SIGTERM signal
[2024-10-27T20:26:14.522+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=holiday_etl_pipeline, task_id=extract_to_landing, execution_date=20241026T000000, start_date=20241027T202559, end_date=20241027T202614
[2024-10-27T20:26:14.534+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 3 for task extract_to_landing (Erro ao criar sessão Spark: Task received SIGTERM signal; 82)
[2024-10-27T20:26:14.716+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=84, status='terminated', started='20:25:58') (84) terminated with exit code None
[2024-10-27T20:26:14.717+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=82, status='terminated', exitcode=1, started='20:25:58') (82) terminated with exit code 1
[2024-10-27T20:30:58.137+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: holiday_etl_pipeline.extract_to_landing scheduled__2024-10-26T00:00:00+00:00 [queued]>
[2024-10-27T20:30:58.145+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: holiday_etl_pipeline.extract_to_landing scheduled__2024-10-26T00:00:00+00:00 [queued]>
[2024-10-27T20:30:58.146+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 2
[2024-10-27T20:30:58.236+0000] {taskinstance.py:2191} INFO - Executing <Task(_PythonDecoratedOperator): extract_to_landing> on 2024-10-26 00:00:00+00:00
[2024-10-27T20:30:58.241+0000] {standard_task_runner.py:60} INFO - Started process 75 to run task
[2024-10-27T20:30:58.244+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'holiday_etl_pipeline', 'extract_to_landing', 'scheduled__2024-10-26T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/holidays_orchestror.py', '--cfg-path', '/tmp/tmpk4oz83gz']
[2024-10-27T20:30:58.245+0000] {standard_task_runner.py:88} INFO - Job 3: Subtask extract_to_landing
[2024-10-27T20:30:58.280+0000] {task_command.py:423} INFO - Running <TaskInstance: holiday_etl_pipeline.extract_to_landing scheduled__2024-10-26T00:00:00+00:00 [running]> on host 85554215aad0
[2024-10-27T20:30:58.336+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='holiday_etl_pipeline' AIRFLOW_CTX_TASK_ID='extract_to_landing' AIRFLOW_CTX_EXECUTION_DATE='2024-10-26T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-10-26T00:00:00+00:00'
[2024-10-27T20:31:13.373+0000] {local_task_job_runner.py:302} WARNING - State of this instance has been externally set to failed. Terminating instance.
[2024-10-27T20:31:13.376+0000] {process_utils.py:131} INFO - Sending 15 to group 75. PIDs of all processes in the group: [77, 75]
[2024-10-27T20:31:13.376+0000] {process_utils.py:86} INFO - Sending the signal 15 to group 75
[2024-10-27T20:31:13.377+0000] {taskinstance.py:2450} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-10-27T20:31:13.378+0000] {spark_session.py:55} ERROR - Erro ao criar sessão Spark: Task received SIGTERM signal
[2024-10-27T20:31:13.379+0000] {spark_session.py:56} ERROR - Python Path: /usr/local/bin/python
[2024-10-27T20:31:13.379+0000] {spark_session.py:57} ERROR - Environment: environ({'AIRFLOW__CELERY__RESULT_BACKEND': 'db+postgresql://***:***@postgres/***', 'DUMB_INIT_SETSID': '0', 'HOSTNAME': '85554215aad0', 'PYTHON_VERSION': '3.11.8', 'LANGUAGE': 'C.UTF-8', 'JAVA_HOME': '/usr/lib/jvm/java-17-openjdk-amd64', 'AIRFLOW_USER_HOME_DIR': '/home/***', 'ADDITIONAL_RUNTIME_APT_DEPS': '', 'PWD': '/opt/***', 'AIRFLOW__CELERY__BROKER_URL': 'redis://:@redis:6379/0', 'AIRFLOW_VERSION': '2.8.1', 'AIRFLOW__CORE__LOAD_EXAMPLES': 'false', 'AIRFLOW__API__AUTH_BACKENDS': '***.api.auth.backend.basic_auth,***.api.auth.backend.session', 'INSTALL_MSSQL_CLIENT': 'true', 'PYTHON_SETUPTOOLS_VERSION': '65.5.1', 'INSTALL_MYSQL_CLIENT_TYPE': 'mariadb', 'GUNICORN_CMD_ARGS': '--worker-tmp-dir /dev/shm', 'LD_PRELOAD': '/usr/lib/x86_64-linux-gnu/libstdc++.so.6', 'HOME': '/home/***', 'LANG': 'C.UTF-8', 'AIRFLOW_HOME': '/opt/***', 'GPG_KEY': 'A035C8C19219BA821ECEA86B64E628F8D684696D', 'AIRFLOW__DATABASE__SQL_ALCHEMY_CONN': 'postgresql+psycopg2://***:***@postgres/***', 'AIRFLOW__CORE__EXECUTOR': 'CeleryExecutor', 'COMMIT_SHA': 'c0ffa9c5d96625c68ded9562632674ed366b5eb3', 'AIRFLOW_PIP_VERSION': '23.3.2', 'AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION': 'true', 'ADDITIONAL_RUNTIME_APT_COMMAND': '', 'INSTALL_POSTGRES_CLIENT': 'true', 'SHLVL': '0', 'LC_MESSAGES': 'C.UTF-8', 'RUNTIME_APT_DEPS': '', 'PYTHON_PIP_VERSION': '24.0', 'RUNTIME_APT_COMMAND': 'echo', 'LD_LIBRARY_PATH': '/usr/local/lib', 'LC_CTYPE': 'C.UTF-8', 'AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK': 'true', 'PYTHON_GET_PIP_SHA256': 'dfe9fd5c28dc98b5ac17979a953ea550cec37ae1b47a5116007395bfacff2ab9', 'AIRFLOW_INSTALLATION_METHOD': '', 'LC_ALL': 'C.UTF-8', 'PYTHON_GET_PIP_URL': 'https://github.com/pypa/get-pip/raw/dbf0c85f76fb6e1ab42aa672ffca6f0a675d9ee4/public/get-pip.py', 'INSTALL_MYSQL_CLIENT': 'true', 'PATH': '/root/bin:/home/***/.local/bin:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-17-openjdk-amd64/bin', 'PYTHON_BASE_IMAGE': 'python:3.11-slim-bookworm', 'AIRFLOW_UID': '50000', 'BUILD_ID': '', 'AIRFLOW__CORE__FERNET_KEY': '', 'DEBIAN_FRONTEND': 'noninteractive', 'AIRFLOW_CONN_SPARK_DEFAULT': 'spark://spark-master:7077', '_MP_FORK_LOGLEVEL_': '20', '_MP_FORK_LOGFILE_': '', '_MP_FORK_LOGFORMAT_': '[%(asctime)s: %(levelname)s/%(processName)s] %(message)s', 'CELERY_LOG_LEVEL': '20', 'CELERY_LOG_FILE': '', 'CELERY_LOG_REDIRECT': '1', 'CELERY_LOG_REDIRECT_LEVEL': 'WARNING', '_AIRFLOW_PARSING_CONTEXT_DAG_ID': 'holiday_etl_pipeline', '_AIRFLOW_PARSING_CONTEXT_TASK_ID': 'extract_to_landing', 'AIRFLOW_CTX_DAG_OWNER': '***', 'AIRFLOW_CTX_DAG_ID': 'holiday_etl_pipeline', 'AIRFLOW_CTX_TASK_ID': 'extract_to_landing', 'AIRFLOW_CTX_EXECUTION_DATE': '2024-10-26T00:00:00+00:00', 'AIRFLOW_CTX_TRY_NUMBER': '1', 'AIRFLOW_CTX_DAG_RUN_ID': 'scheduled__2024-10-26T00:00:00+00:00', 'PYSPARK_PYTHON': '/usr/local/bin/python', 'PYSPARK_DRIVER_PYTHON': '/usr/local/bin/python'})
[2024-10-27T20:31:13.380+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/opt/airflow/dags/conf/spark_session.py", line 45, in get_session
    cls._instance = builder.getOrCreate()
                    ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/sql/session.py", line 477, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/context.py", line 512, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/context.py", line 198, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/context.py", line 432, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
                                       ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/java_gateway.py", line 103, in launch_gateway
    time.sleep(0.1)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 2452, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/holidays_orchestror.py", line 39, in extract_holidays
    return HolidayLandingOrchestrator.extract_holidays()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/scripts/landing/holliday_landing.py", line 255, in extract_holidays
    extractor = HolidayLanding()
                ^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/scripts/landing/holliday_landing.py", line 67, in __init__
    self.spark = SparkSessionManager.get_session()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/conf/spark_session.py", line 58, in get_session
    raise Exception(error_msg)
Exception: Erro ao criar sessão Spark: Task received SIGTERM signal
[2024-10-27T20:31:13.396+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=holiday_etl_pipeline, task_id=extract_to_landing, execution_date=20241026T000000, start_date=20241027T203058, end_date=20241027T203113
[2024-10-27T20:31:13.407+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 3 for task extract_to_landing (Erro ao criar sessão Spark: Task received SIGTERM signal; 75)
[2024-10-27T20:31:13.429+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=77, status='terminated', started='20:30:57') (77) terminated with exit code None
[2024-10-27T20:31:13.430+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=75, status='terminated', exitcode=1, started='20:30:57') (75) terminated with exit code 1
[2024-10-27T21:14:32.123+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: holiday_etl_pipeline.extract_to_landing scheduled__2024-10-26T00:00:00+00:00 [queued]>
[2024-10-27T21:14:32.129+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: holiday_etl_pipeline.extract_to_landing scheduled__2024-10-26T00:00:00+00:00 [queued]>
[2024-10-27T21:14:32.130+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 2
[2024-10-27T21:14:32.138+0000] {taskinstance.py:2191} INFO - Executing <Task(_PythonDecoratedOperator): extract_to_landing> on 2024-10-26 00:00:00+00:00
[2024-10-27T21:14:32.142+0000] {standard_task_runner.py:60} INFO - Started process 90 to run task
[2024-10-27T21:14:32.145+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'holiday_etl_pipeline', 'extract_to_landing', 'scheduled__2024-10-26T00:00:00+00:00', '--job-id', '20', '--raw', '--subdir', 'DAGS_FOLDER/holidays_orchestror.py', '--cfg-path', '/tmp/tmp3c06akre']
[2024-10-27T21:14:32.147+0000] {standard_task_runner.py:88} INFO - Job 20: Subtask extract_to_landing
[2024-10-27T21:14:32.185+0000] {task_command.py:423} INFO - Running <TaskInstance: holiday_etl_pipeline.extract_to_landing scheduled__2024-10-26T00:00:00+00:00 [running]> on host f28df99dfa84
[2024-10-27T21:14:32.243+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='holiday_etl_pipeline' AIRFLOW_CTX_TASK_ID='extract_to_landing' AIRFLOW_CTX_EXECUTION_DATE='2024-10-26T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-10-26T00:00:00+00:00'
[2024-10-27T21:14:47.266+0000] {local_task_job_runner.py:302} WARNING - State of this instance has been externally set to failed. Terminating instance.
[2024-10-27T21:14:47.268+0000] {process_utils.py:131} INFO - Sending 15 to group 90. PIDs of all processes in the group: [91, 90]
[2024-10-27T21:14:47.269+0000] {process_utils.py:86} INFO - Sending the signal 15 to group 90
[2024-10-27T21:14:47.269+0000] {taskinstance.py:2450} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-10-27T21:14:47.270+0000] {spark_session.py:55} ERROR - Erro ao criar sessão Spark: Task received SIGTERM signal
[2024-10-27T21:14:47.270+0000] {spark_session.py:56} ERROR - Python Path: /usr/local/bin/python
[2024-10-27T21:14:47.271+0000] {spark_session.py:57} ERROR - Environment: environ({'AIRFLOW__CELERY__RESULT_BACKEND': 'db+postgresql://***:***@postgres/***', 'DUMB_INIT_SETSID': '0', 'HOSTNAME': 'f28df99dfa84', 'PYTHON_VERSION': '3.11.8', 'LANGUAGE': 'C.UTF-8', 'JAVA_HOME': '/usr/lib/jvm/java-17-openjdk-amd64', 'AIRFLOW_USER_HOME_DIR': '/home/***', 'ADDITIONAL_RUNTIME_APT_DEPS': '', 'PWD': '/opt/***', 'AIRFLOW__CELERY__BROKER_URL': 'redis://:@redis:6379/0', 'AIRFLOW_VERSION': '2.8.1', 'AIRFLOW__CORE__LOAD_EXAMPLES': 'false', 'AIRFLOW__API__AUTH_BACKENDS': '***.api.auth.backend.basic_auth,***.api.auth.backend.session', 'INSTALL_MSSQL_CLIENT': 'true', 'PYTHON_SETUPTOOLS_VERSION': '65.5.1', 'INSTALL_MYSQL_CLIENT_TYPE': 'mariadb', 'GUNICORN_CMD_ARGS': '--worker-tmp-dir /dev/shm', 'LD_PRELOAD': '/usr/lib/x86_64-linux-gnu/libstdc++.so.6', 'HOME': '/home/***', 'LANG': 'C.UTF-8', 'AIRFLOW_HOME': '/opt/***', 'GPG_KEY': 'A035C8C19219BA821ECEA86B64E628F8D684696D', 'AIRFLOW__DATABASE__SQL_ALCHEMY_CONN': 'postgresql+psycopg2://***:***@postgres/***', 'AIRFLOW__CORE__EXECUTOR': 'CeleryExecutor', 'COMMIT_SHA': 'c0ffa9c5d96625c68ded9562632674ed366b5eb3', 'AIRFLOW_PIP_VERSION': '23.3.2', 'AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION': 'true', 'ADDITIONAL_RUNTIME_APT_COMMAND': '', 'INSTALL_POSTGRES_CLIENT': 'true', 'SHLVL': '0', 'LC_MESSAGES': 'C.UTF-8', 'RUNTIME_APT_DEPS': '', 'PYTHON_PIP_VERSION': '24.0', 'RUNTIME_APT_COMMAND': 'echo', 'LD_LIBRARY_PATH': '/usr/local/lib', 'LC_CTYPE': 'C.UTF-8', 'AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK': 'true', 'PYTHON_GET_PIP_SHA256': 'dfe9fd5c28dc98b5ac17979a953ea550cec37ae1b47a5116007395bfacff2ab9', 'AIRFLOW_INSTALLATION_METHOD': '', 'LC_ALL': 'C.UTF-8', 'PYTHON_GET_PIP_URL': 'https://github.com/pypa/get-pip/raw/dbf0c85f76fb6e1ab42aa672ffca6f0a675d9ee4/public/get-pip.py', 'INSTALL_MYSQL_CLIENT': 'true', 'PATH': '/root/bin:/home/***/.local/bin:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-17-openjdk-amd64/bin', 'PYTHON_BASE_IMAGE': 'python:3.11-slim-bookworm', 'AIRFLOW_UID': '50000', 'BUILD_ID': '', 'AIRFLOW__CORE__FERNET_KEY': '', 'DEBIAN_FRONTEND': 'noninteractive', 'AIRFLOW_CONN_SPARK_DEFAULT': 'spark://spark-master:7077', '_MP_FORK_LOGLEVEL_': '20', '_MP_FORK_LOGFILE_': '', '_MP_FORK_LOGFORMAT_': '[%(asctime)s: %(levelname)s/%(processName)s] %(message)s', 'CELERY_LOG_LEVEL': '20', 'CELERY_LOG_FILE': '', 'CELERY_LOG_REDIRECT': '1', 'CELERY_LOG_REDIRECT_LEVEL': 'WARNING', '_AIRFLOW_PARSING_CONTEXT_DAG_ID': 'holiday_etl_pipeline', '_AIRFLOW_PARSING_CONTEXT_TASK_ID': 'extract_to_landing', 'AIRFLOW_CTX_DAG_OWNER': '***', 'AIRFLOW_CTX_DAG_ID': 'holiday_etl_pipeline', 'AIRFLOW_CTX_TASK_ID': 'extract_to_landing', 'AIRFLOW_CTX_EXECUTION_DATE': '2024-10-26T00:00:00+00:00', 'AIRFLOW_CTX_TRY_NUMBER': '1', 'AIRFLOW_CTX_DAG_RUN_ID': 'scheduled__2024-10-26T00:00:00+00:00', 'PYSPARK_PYTHON': '/usr/local/bin/python', 'PYSPARK_DRIVER_PYTHON': '/usr/local/bin/python'})
[2024-10-27T21:14:47.271+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/opt/airflow/dags/conf/spark_session.py", line 45, in get_session
    cls._instance = builder.getOrCreate()
                    ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/sql/session.py", line 477, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/context.py", line 512, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/context.py", line 198, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/context.py", line 432, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
                                       ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/java_gateway.py", line 103, in launch_gateway
    time.sleep(0.1)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 2452, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/holidays_orchestror.py", line 39, in extract_holidays
    return HolidayLandingOrchestrator.extract_holidays()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/scripts/landing/holliday_landing.py", line 201, in extract_holidays
    extractor = HolidayLanding()
                ^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/scripts/landing/holliday_landing.py", line 68, in __init__
    self.spark = SparkSessionManager.get_session()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/conf/spark_session.py", line 58, in get_session
    raise Exception(error_msg)
Exception: Erro ao criar sessão Spark: Task received SIGTERM signal
[2024-10-27T21:14:47.283+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=holiday_etl_pipeline, task_id=extract_to_landing, execution_date=20241026T000000, start_date=20241027T211432, end_date=20241027T211447
[2024-10-27T21:14:47.295+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 20 for task extract_to_landing (Erro ao criar sessão Spark: Task received SIGTERM signal; 90)
[2024-10-27T21:14:47.321+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=90, status='terminated', exitcode=1, started='21:14:31') (90) terminated with exit code 1
[2024-10-27T21:14:47.322+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=91, status='terminated', started='21:14:31') (91) terminated with exit code None
[2024-10-27T21:17:10.641+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: holiday_etl_pipeline.extract_to_landing scheduled__2024-10-26T00:00:00+00:00 [queued]>
[2024-10-27T21:17:10.648+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: holiday_etl_pipeline.extract_to_landing scheduled__2024-10-26T00:00:00+00:00 [queued]>
[2024-10-27T21:17:10.649+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 2
[2024-10-27T21:17:10.660+0000] {taskinstance.py:2191} INFO - Executing <Task(_PythonDecoratedOperator): extract_to_landing> on 2024-10-26 00:00:00+00:00
[2024-10-27T21:17:10.664+0000] {standard_task_runner.py:60} INFO - Started process 83 to run task
[2024-10-27T21:17:10.668+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'holiday_etl_pipeline', 'extract_to_landing', 'scheduled__2024-10-26T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/holidays_orchestror.py', '--cfg-path', '/tmp/tmph_zx3vci']
[2024-10-27T21:17:10.671+0000] {standard_task_runner.py:88} INFO - Job 3: Subtask extract_to_landing
[2024-10-27T21:17:10.715+0000] {task_command.py:423} INFO - Running <TaskInstance: holiday_etl_pipeline.extract_to_landing scheduled__2024-10-26T00:00:00+00:00 [running]> on host 2a7fac9bc479
[2024-10-27T21:17:10.792+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='holiday_etl_pipeline' AIRFLOW_CTX_TASK_ID='extract_to_landing' AIRFLOW_CTX_EXECUTION_DATE='2024-10-26T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-10-26T00:00:00+00:00'
[2024-10-27T21:17:25.787+0000] {local_task_job_runner.py:302} WARNING - State of this instance has been externally set to success. Terminating instance.
[2024-10-27T21:17:25.790+0000] {process_utils.py:131} INFO - Sending 15 to group 83. PIDs of all processes in the group: [84, 83]
[2024-10-27T21:17:25.794+0000] {process_utils.py:86} INFO - Sending the signal 15 to group 83
[2024-10-27T21:17:25.795+0000] {taskinstance.py:2450} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-10-27T21:17:25.796+0000] {spark_session.py:55} ERROR - Erro ao criar sessão Spark: Task received SIGTERM signal
[2024-10-27T21:17:25.796+0000] {spark_session.py:56} ERROR - Python Path: /usr/local/bin/python
[2024-10-27T21:17:25.796+0000] {spark_session.py:57} ERROR - Environment: environ({'AIRFLOW__CELERY__RESULT_BACKEND': 'db+postgresql://***:***@postgres/***', 'DUMB_INIT_SETSID': '0', 'HOSTNAME': '2a7fac9bc479', 'PYTHON_VERSION': '3.11.8', 'LANGUAGE': 'C.UTF-8', 'JAVA_HOME': '/usr/lib/jvm/java-17-openjdk-amd64', 'AIRFLOW_USER_HOME_DIR': '/home/***', 'ADDITIONAL_RUNTIME_APT_DEPS': '', 'PWD': '/opt/***', 'AIRFLOW__CELERY__BROKER_URL': 'redis://:@redis:6379/0', 'AIRFLOW_VERSION': '2.8.1', 'AIRFLOW__CORE__LOAD_EXAMPLES': 'false', 'AIRFLOW__API__AUTH_BACKENDS': '***.api.auth.backend.basic_auth,***.api.auth.backend.session', 'INSTALL_MSSQL_CLIENT': 'true', 'PYTHON_SETUPTOOLS_VERSION': '65.5.1', 'INSTALL_MYSQL_CLIENT_TYPE': 'mariadb', 'GUNICORN_CMD_ARGS': '--worker-tmp-dir /dev/shm', 'LD_PRELOAD': '/usr/lib/x86_64-linux-gnu/libstdc++.so.6', 'HOME': '/home/***', 'LANG': 'C.UTF-8', 'AIRFLOW_HOME': '/opt/***', 'GPG_KEY': 'A035C8C19219BA821ECEA86B64E628F8D684696D', 'AIRFLOW__DATABASE__SQL_ALCHEMY_CONN': 'postgresql+psycopg2://***:***@postgres/***', 'AIRFLOW__CORE__EXECUTOR': 'CeleryExecutor', 'COMMIT_SHA': 'c0ffa9c5d96625c68ded9562632674ed366b5eb3', 'AIRFLOW_PIP_VERSION': '23.3.2', 'AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION': 'true', 'ADDITIONAL_RUNTIME_APT_COMMAND': '', 'INSTALL_POSTGRES_CLIENT': 'true', 'SHLVL': '0', 'LC_MESSAGES': 'C.UTF-8', 'RUNTIME_APT_DEPS': '', 'PYTHON_PIP_VERSION': '24.0', 'RUNTIME_APT_COMMAND': 'echo', 'LD_LIBRARY_PATH': '/usr/local/lib', 'LC_CTYPE': 'C.UTF-8', 'AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK': 'true', 'PYTHON_GET_PIP_SHA256': 'dfe9fd5c28dc98b5ac17979a953ea550cec37ae1b47a5116007395bfacff2ab9', 'AIRFLOW_INSTALLATION_METHOD': '', 'LC_ALL': 'C.UTF-8', 'PYTHON_GET_PIP_URL': 'https://github.com/pypa/get-pip/raw/dbf0c85f76fb6e1ab42aa672ffca6f0a675d9ee4/public/get-pip.py', 'INSTALL_MYSQL_CLIENT': 'true', 'PATH': '/root/bin:/home/***/.local/bin:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-17-openjdk-amd64/bin', 'PYTHON_BASE_IMAGE': 'python:3.11-slim-bookworm', 'AIRFLOW_UID': '50000', 'BUILD_ID': '', 'AIRFLOW__CORE__FERNET_KEY': '', 'DEBIAN_FRONTEND': 'noninteractive', 'AIRFLOW_CONN_SPARK_DEFAULT': 'spark://spark-master:7077', '_MP_FORK_LOGLEVEL_': '20', '_MP_FORK_LOGFILE_': '', '_MP_FORK_LOGFORMAT_': '[%(asctime)s: %(levelname)s/%(processName)s] %(message)s', 'CELERY_LOG_LEVEL': '20', 'CELERY_LOG_FILE': '', 'CELERY_LOG_REDIRECT': '1', 'CELERY_LOG_REDIRECT_LEVEL': 'WARNING', '_AIRFLOW_PARSING_CONTEXT_DAG_ID': 'holiday_etl_pipeline', '_AIRFLOW_PARSING_CONTEXT_TASK_ID': 'extract_to_landing', 'AIRFLOW_CTX_DAG_OWNER': '***', 'AIRFLOW_CTX_DAG_ID': 'holiday_etl_pipeline', 'AIRFLOW_CTX_TASK_ID': 'extract_to_landing', 'AIRFLOW_CTX_EXECUTION_DATE': '2024-10-26T00:00:00+00:00', 'AIRFLOW_CTX_TRY_NUMBER': '1', 'AIRFLOW_CTX_DAG_RUN_ID': 'scheduled__2024-10-26T00:00:00+00:00', 'PYSPARK_PYTHON': '/usr/local/bin/python', 'PYSPARK_DRIVER_PYTHON': '/usr/local/bin/python'})
[2024-10-27T21:17:25.797+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/opt/airflow/dags/conf/spark_session.py", line 45, in get_session
    cls._instance = builder.getOrCreate()
                    ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/sql/session.py", line 477, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/context.py", line 512, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/context.py", line 198, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/context.py", line 432, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
                                       ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/java_gateway.py", line 103, in launch_gateway
    time.sleep(0.1)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 2452, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/holidays_orchestror.py", line 39, in extract_holidays
    return HolidayLandingOrchestrator.extract_holidays()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/scripts/landing/holliday_landing.py", line 201, in extract_holidays
    extractor = HolidayLanding()
                ^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/scripts/landing/holliday_landing.py", line 68, in __init__
    self.spark = SparkSessionManager.get_session()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/conf/spark_session.py", line 58, in get_session
    raise Exception(error_msg)
Exception: Erro ao criar sessão Spark: Task received SIGTERM signal
[2024-10-27T21:17:25.813+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=holiday_etl_pipeline, task_id=extract_to_landing, execution_date=20241026T000000, start_date=20241027T211710, end_date=20241027T211725
[2024-10-27T21:17:25.829+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 3 for task extract_to_landing (Erro ao criar sessão Spark: Task received SIGTERM signal; 83)
[2024-10-27T21:17:25.847+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=83, status='terminated', exitcode=1, started='21:17:09') (83) terminated with exit code 1
[2024-10-27T21:17:25.848+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=84, status='terminated', started='21:17:09') (84) terminated with exit code None
